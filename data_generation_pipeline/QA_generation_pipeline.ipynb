{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwndFE1wri_4"
      },
      "source": [
        "### This colab file augment QA dataset questions from sources of **governance-v2**, **whitepaper** and **yield space papers**. It also augment previous document sources via custom prompting to **docs-v2** and **addendum-docs**.\n",
        "\n",
        "### This colab file also experiments with integrated QA-generation pipeline to ensure a smooth and trustworthy data generation flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Nnd3-resaB8"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install langchain===0.0.230 openai chromadb==0.3.26 pydantic==1.10.8 GitPython ipython tiktoken\n",
        "!pip install unstructured==0.8.5 pdf2image pytesseract pypdf\n",
        "!apt-get install -y poppler-utils tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwwGcdRTskP4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.document_loaders import GitLoader, UnstructuredPDFLoader, OnlinePDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQAWithSourcesChain, RetrievalQA\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "from getpass import getpass\n",
        "from pathlib import Path\n",
        "from langchain.callbacks import StdOutCallbackHandler\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import json\n",
        "\n",
        "stdout_handler = StdOutCallbackHandler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvI_QEUkJvQJ"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcDJ2rAZsnoX"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txba4gD_K8U6"
      },
      "source": [
        "## Load up repo and pdf documents and generate questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPFn89XlI6-n"
      },
      "outputs": [],
      "source": [
        "# Load repository with .md files as targets to split and store\n",
        "def load_repo(remote_repo_url, local_repo_path, branch, file_filter=None):\n",
        "    local_repo_exists = Path(local_repo_path).is_dir() # second and later loadings\n",
        "\n",
        "    if local_repo_exists:\n",
        "        loader = GitLoader(\n",
        "            repo_path=local_repo_path,\n",
        "            branch=branch,\n",
        "            file_filter=file_filter\n",
        "        )\n",
        "    else:\n",
        "        loader = GitLoader(\n",
        "            clone_url=remote_repo_url,\n",
        "            repo_path=local_repo_path,\n",
        "            branch=branch,\n",
        "            file_filter=file_filter\n",
        "        )\n",
        "    return loader.load() # load the required source files\n",
        "\n",
        "# Load PDF documents\n",
        "def load_pdf(file_path):\n",
        "    loader = UnstructuredPDFLoader(file_path)\n",
        "    return loader.load()\n",
        "\n",
        "def load_online_pdf(file_url):\n",
        "    loader = OnlinePDFLoader(file_url)\n",
        "    return loader.load()\n",
        "\n",
        "# Load single md document from drive/filesystem\n",
        "def load_md(markdown_path):\n",
        "    loader = TextLoader(markdown_path) # raw, do not parse md. https://github.com/langchain-ai/langchain/issues/3591\n",
        "    return loader.load()\n",
        "\n",
        "def split_docs(docs, language, chunk_size, chunk_overlap, force_category):\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_language(language=language, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "    all_splits=[]\n",
        "    all_metadatas=[]\n",
        "    for d in docs:\n",
        "        doc_file = d.page_content\n",
        "        metadata = d.metadata # metadata including filepath, filename, other stuffs\n",
        "        splits = text_splitter.split_text(doc_file) # parse the document to small chunk of code snippets\n",
        "\n",
        "        metadata['category'] = force_category\n",
        "\n",
        "        # this is only for governance type questions\n",
        "        if metadata['source'].split('.')[-1] == 'md':\n",
        "          title = f\"# {metadata['file_name'].split('.')[0]}\"\n",
        "          for i in range(len(splits)):\n",
        "            splits[i] = f\"{title}\\n\\n{splits[i]}\" # string is immutable, must write explitly\n",
        "\n",
        "\n",
        "        metadatas = [metadata for _ in splits]\n",
        "        all_splits += splits\n",
        "        all_metadatas += metadatas # collecting metadata headers and the splited chunks from each document.\n",
        "\n",
        "    return {\n",
        "        'all_splits': all_splits,\n",
        "        'all_metadatas': all_metadatas # pack as dict/json\n",
        "    }\n",
        "\n",
        "def split_pdf(docs, chunk_size, chunk_overlap, force_category):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap) # no language as pdf is parsed as plain txt\n",
        "\n",
        "    all_splits=[]\n",
        "    all_metadatas=[]\n",
        "    for d in docs:\n",
        "        doc_file=d.page_content\n",
        "        metadata = d.metadata # metadata including filepath, filename, other stuffs\n",
        "        splits = text_splitter.split_text(doc_file) # parse the document to small chunk of code snippets\n",
        "\n",
        "        metadata['category'] = force_category\n",
        "\n",
        "        metadatas = [metadata for _ in splits]\n",
        "        all_splits += splits\n",
        "        all_metadatas += metadatas # collecting metadata headers and the splited chunks from each document.\n",
        "\n",
        "    return {\n",
        "        'all_splits': all_splits,\n",
        "        'all_metadatas': all_metadatas # pack as dict/json\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vW6bFagFAPA"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "def split_md(docs, chunk_size, chunk_overlap, force_category):\n",
        "    text_splitter = MarkdownTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "    all_splits=[]\n",
        "    all_metadatas=[]\n",
        "    for d in docs:\n",
        "        doc_file = d.page_content\n",
        "        metadata = d.metadata # metadata including filepath, filename, other stuffs\n",
        "        splits = text_splitter.split_text(doc_file) # parse the document to small chunk of code snippets\n",
        "\n",
        "        metadata['category'] = force_category\n",
        "\n",
        "        metadatas = [metadata for _ in splits]\n",
        "        all_splits += splits\n",
        "        all_metadatas += metadatas # collecting metadata headers and the splited chunks from each document.\n",
        "\n",
        "    return {\n",
        "        'all_splits': all_splits,\n",
        "        'all_metadatas': all_metadatas # pack as dict/json\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOR3ppgyJ0mW"
      },
      "outputs": [],
      "source": [
        "# documentation repo\n",
        "remote_repo_docsV2_url=\"https://github.com/yieldprotocol/docs-v2\" # the docs all coming from the github repo (public)\n",
        "local_repo_docsV2_path=\"/tmp/yield_docs_v2_repo\"\n",
        "\n",
        "# cookbook repo\n",
        "remote_repo_addendum_url=\"https://github.com/yieldprotocol/addendum-docs\"\n",
        "local_repo_addendum_path=\"/tmp/yield_addendum-docs\"\n",
        "\n",
        "# governance repo, the documents under this repo is kinda problematic, does not have a lot of content, most are link to external files\n",
        "# thinking of whether to load the files affected by governance-v2 as well?\n",
        "remote_repo_governance_url = \"https://github.com/yieldprotocol/governance-v2\"\n",
        "local_repo_governance_path = \"/tmp/governance-v2\"\n",
        "\n",
        "branch=\"main\" # specify the branch of the remote repo urls\n",
        "file_filter=lambda file_path: file_path.endswith(\".md\") # select only .md files as 'documentations'\n",
        "\n",
        "chunk_size_chars = 2000 # context length required, in SFT chunk it to 1000 at evaluation\n",
        "chunk_overlap_chars = 0 # no overlapping since containing code chunk\n",
        "\n",
        "documentation_docs = load_repo(remote_repo_docsV2_url, local_repo_docsV2_path, branch, file_filter)\n",
        "addendum_docs = load_repo(remote_repo_addendum_url, local_repo_addendum_path, branch, file_filter)\n",
        "governance_docs = load_repo(remote_repo_governance_url, local_repo_governance_path, branch, file_filter)\n",
        "\n",
        "documentation_splits = split_docs(documentation_docs, Language.MARKDOWN, chunk_size_chars, chunk_overlap_chars, force_category='general')\n",
        "addendum_splits = split_docs(addendum_docs, Language.MARKDOWN, chunk_size_chars, chunk_overlap_chars, force_category='technical')\n",
        "governance_splits = split_docs(governance_docs, Language.MARKDOWN, chunk_size_chars, chunk_overlap_chars, force_category='governance')\n",
        "\n",
        "# trim down first and last file of gov_splits, as there are placeholders\n",
        "governance_splits['all_splits'] = governance_splits['all_splits'][1:]\n",
        "governance_splits['all_metadatas'] = governance_splits['all_metadatas'][1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDeqn0V91IHI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "whitepaper_data = load_online_pdf(file_url=\"https://yieldprotocol.com/Yield.pdf\")\n",
        "yieldspace_data = load_md(markdown_path=\"/content/drive/MyDrive/YieldSpace.md\")\n",
        "\n",
        "# sanitize the source\n",
        "whitepaper_data[0].metadata['source'] = \"https://yieldprotocol.com/Yield.pdf\"\n",
        "yieldspace_data[0].metadata['source'] = \"https://yieldprotocol.com/YieldSpace.pdf\"\n",
        "\n",
        "whitepaper_splits = split_pdf(whitepaper_data, 2000, chunk_overlap_chars, force_category=\"paper\")\n",
        "yieldspace_splits = split_md(yieldspace_data, 2500, 0, force_category=\"paper\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84sFt4nytpI1"
      },
      "outputs": [],
      "source": [
        "for i, split in enumerate(whitepaper_splits['all_splits']):\n",
        "  display(Markdown(f\"# Chunk {i+1}\"))\n",
        "  display(Markdown(split))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9swI6iL3QzVT"
      },
      "outputs": [],
      "source": [
        "vector_db = Chroma(embedding_function=OpenAIEmbeddings())\n",
        "# chunking them already\n",
        "documentation_splits_ids = vector_db.add_texts(documentation_splits['all_splits'], documentation_splits['all_metadatas'])\n",
        "addendum_splits_ids = vector_db.add_texts(addendum_splits['all_splits'], addendum_splits['all_metadatas'])\n",
        "governance_splits_ids = vector_db.add_texts(governance_splits['all_splits'], governance_splits['all_metadatas'])\n",
        "whitepaper_splits_ids = vector_db.add_texts(whitepaper_splits['all_splits'], whitepaper_splits['all_metadatas'])\n",
        "yieldspace_splits_ids = vector_db.add_texts(yieldspace_splits['all_splits'], yieldspace_splits['all_metadatas'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LShF3VBwLKZ"
      },
      "outputs": [],
      "source": [
        "# You are a Web3 user who wants to learn about the Yield protocol and its inner workings, concepts and methods to integrate using both client-side and smart contract code.\n",
        "\n",
        "question_bank_prompt_template = \"\"\"\n",
        "You are an inquisitive Web3 user who wants to learn about the YieldSpace paper released by Yield Protocol that explains the mathematical foundations of YieldSpace.\n",
        "\n",
        "# INSTRUCTIONS\n",
        "- Generate 5 questions to ask about the protocol from the document provided below, always ensure the questions are related to the document text and do not make up any information.\n",
        "- When you ask a question, make sure that you can also find evidences in the document that makes it answerable, but do not attempt to answer.\n",
        "- The questions should be in plain text as much as possible, when the question contains latex formula, write the latex symbols in double slash '\\\\\\\\' to avoid json decoder error.\n",
        "- Organize the results in lines, each question takes a line. Do not enumerate the questions, just write them to lines.\n",
        "\n",
        "# DOCUMENT\n",
        "{document}\n",
        "\n",
        "# RESULT\"\"\"\n",
        "\n",
        "QUESTION_BANK_PROMPT = PromptTemplate(\n",
        "    template=question_bank_prompt_template, input_variables=[\"document\"]\n",
        ")\n",
        "\n",
        "# one single document (chunk) and asking 5 questions\n",
        "# 16k context is also available.\n",
        "question_bank_llm_chain = LLMChain(\n",
        "    llm=ChatOpenAI(temperature=0.10, model=\"gpt-4\"),\n",
        "    prompt=QUESTION_BANK_PROMPT\n",
        ")\n",
        "\n",
        "# code-related hallucination --> maybe turned it off when the first tested finetuned LLAMA-V2 is tested out.\n",
        "\n",
        "answer_prompt_template = \"\"\"\n",
        "You are a Web3 expert who is able to answer any user query on Yield protocol's documentation, code, whitepapers and many other such topics.\n",
        "\n",
        "# INSTRUCTIONS\n",
        "- The user's query will be wrapped in triple back ticks\n",
        "- Only answer the query using the provided context below which may include general information and code, do not make up any information.\n",
        "- If you want to explicitly support your answer from the context, quote the part in the document in your answer.\n",
        "- If the query is related to code or integration, provide a step by step explanation on the process, use code suggestions whereever relevant and always use markdown format annotated with the language to show the code.\n",
        "- If the query is related to mathematic formulas or mathematic proofs, explain in natural languguage in an easy-to-understand manner and use markdown/latex to compose math formulas if applicable.\n",
        "- For code suggestions, use comments to explain each important concept, class, variable, function and parameter.\n",
        "- However, if you could not find documented code reference in the context, do not attempt to deliniate your answer in code, instead, use natural language supported from the context retrieved.\n",
        "- Yield Protocol has no JS SDK so always use ethers package for JS code suggestions.\n",
        "\n",
        "# CONTEXT\n",
        "{augment_context}\n",
        "{context}\n",
        "\n",
        "# QUERY\n",
        "```\n",
        "{question}\n",
        "```\n",
        "\"\"\"\n",
        "ANSWER_PROMPT = PromptTemplate(\n",
        "    template=answer_prompt_template, input_variables=[\"context\", \"augment_context\", \"question\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbPEtpbwKzLO"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks.manager import (\n",
        "    AsyncCallbackManagerForToolRun,\n",
        "    CallbackManagerForToolRun,\n",
        ")\n",
        "from langchain.agents import AgentType, initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
        "from typing import Optional, Type\n",
        "\n",
        "complex_template = \"\"\"\n",
        "You are a web3 expert using Yield protocol and its services. {role_description}\n",
        "Write me engaging and profound case study questions rooted in real life web3 user experience scenario using first-person pronoun (I) in the text, based on the provided documents.\n",
        "\n",
        "# INSTRUCTIONS\n",
        "- Compose {k} such long, qualitative and engaging real-life case study question that is arisen from the following document.\n",
        "- Always ensure the questions are related to the document text and do not make up any information.\n",
        "- Before you ask a question, make sure that you can also find evidences in the document in the form of sentences, phrases, or function descriptions that makes it answerable.\n",
        "- Organize the results in lines, each question or each evidence takes a line. Do not enumerate the questions and evidences, just write them to lines.\n",
        "\n",
        "# DOCUMENT\n",
        "{document}\n",
        "\n",
        "# EXAMPLE RESULT\n",
        "evidence: <evidence 1>\n",
        "evidence: <evidence 2>\n",
        "evidence: <evidence 3>\n",
        "...\n",
        "question: <question 1>\n",
        "question: <question 2>\n",
        "question: <question 3>\n",
        "...\n",
        "\n",
        "# YOUR RESULT\n",
        "\"\"\"\n",
        "\n",
        "COMPLEX_QUESTION_PROMPT = PromptTemplate(\n",
        "    template=complex_template, input_variables=[\"document\", \"role_description\", \"k\"]\n",
        ")\n",
        "\n",
        "role_dict = {\n",
        "    \"doc\" : \"Here is a piece of document located at Yield Protocol's website and you have questions on it.\",\n",
        "    \"addendum\" : \"Here is a piece of techinical document related to protocol operations. You are curious on how these code snippets work.\",\n",
        "    \"governance\" : \"Here is a governance proposal proposing changes in the protocol. Remember that proposal numbers comes in chronologicial order, with larger number being recent proposals, and smaller number are old proposals.\",\n",
        "    \"whitepaper\" : \"You are looking at its whitepaper and you want to talk to the authors of this paper to discuss about the details in this paper.\",\n",
        "    \"yieldspace\" : \"You are looking at its newly published paper called 'yield space' and you want to talk to the authors of this paper to discuss about the details in this paper. Especially you noticed that this paper is heavily focused on mathematical formulas related to the concept of 'invariants' in decentralized finance. You are very interested in studying the maths behind this paper, pay special attention to the numeric relationships between objects in this passage.\"\n",
        "}\n",
        "\n",
        "# def ask_complex_question(document, role, k=\"3\"):\n",
        "#   complex_question_llm_chain = LLMChain(\n",
        "#       llm=llm,\n",
        "#       prompt=COMPLEX_QUESTION_PROMPT\n",
        "#   )\n",
        "#   print(f\"Document:{document}\")\n",
        "#   print(f\"Role: {role_dict[role]}\")\n",
        "#   print(f\"k: {k}\")\n",
        "#   result = complex_question_llm_chain.run(document, role_description=role_dict[role], k=k)\n",
        "#   return result\n",
        "\n",
        "# question_writer = StructuredTool.from_function(\n",
        "#         name=\"Question Writer\",\n",
        "#         func=ask_complex_question,\n",
        "#         description=\"useful for when you need to ask engaging and high-quality questions on the given document in Yield Protocol related stuff.\",\n",
        "#     )\n",
        "\n",
        "complex_answer_prompt_template = \"\"\"\n",
        "You are Yield Protocol's Web3 expert who is able to answer any user query on Yield protocol's documentation, code, whitepapers and many other such topics.\n",
        "\n",
        "# INSTRUCTIONS\n",
        "- The user's query will be wrapped in triple back ticks.\n",
        "- Only answer the query using the provided context below which may include general information and code, do not make up any information.\n",
        "- Aside from the documented context, there's also a list of evidences straightforwardly related to the question asked, if you found the fetched documents are contradicting or confusing, you can use the concise evidences to aid your navigation inside the context.\n",
        "- If you want to support your answer from the context, quote the part in the document in your answer.\n",
        "- If the query is related to code or integration, provide a step by step explanation on the process, use code suggestions whereever relevant and always use markdown format annotated with the language to show the code.\n",
        "- If the query is related to mathematic formulas or mathematic proofs, explain in natural languguage in an easy-to-understand manner and use markdown/latex to compose math formulas if applicable.\n",
        "- For code suggestions, use comments to explain each important concept, class, variable, function and parameter.\n",
        "- However, if you could not find documented code reference in the context, do not attempt to deliniate your answer in code, instead, use natural language supported from the context retrieved.\n",
        "- Yield Protocol has no JS SDK so always use ethers package for JS code suggestions.\n",
        "\n",
        "# EVIDENCE\n",
        "{evidence}\n",
        "\n",
        "# CONTEXT\n",
        "{augment_context}\n",
        "{context}\n",
        "\n",
        "# QUERY\n",
        "```\n",
        "{question}\n",
        "```\n",
        "\"\"\"\n",
        "COMPLEX_ANSWER_PROMPT = PromptTemplate(\n",
        "    template=complex_answer_prompt_template, input_variables=[\"context\", \"augment_context\", \"evidence\", \"question\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZAR8HzN2dvT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "\n",
        "def answer(query, category, injected_doc, top_k=5, show_sources=False):\n",
        "    display(Markdown(f\"# Answering query:\\n{query}\"))\n",
        "\n",
        "    llm_chain = LLMChain(\n",
        "      llm=ChatOpenAI(temperature=0.1, model=\"gpt-4\"),\n",
        "      prompt=ANSWER_PROMPT\n",
        "    )\n",
        "\n",
        "    retriever=vector_db.as_retriever(search_type=\"mmr\", search_kwargs = {\n",
        "      'k': top_k,\n",
        "      'filter': {'category': category}\n",
        "    })\n",
        "\n",
        "    context = retriever.get_relevant_documents(query)\n",
        "    injected_context, injected_meta = injected_doc\n",
        "\n",
        "    # context is multiple, we store them in the dataset for augmenting knowledge purpose (to inject to llama prompts)\n",
        "    ret = {'question': query, 'answer': \"\", 'category': injected_meta['category'], 'contexts': [injected_context] + [con.page_content for con in context]}\n",
        "\n",
        "    with get_openai_callback() as cb:\n",
        "      ans = llm_chain.run(question=query, context=context, augment_context=injected_context)\n",
        "      ret['answer'] = ans\n",
        "      display(Markdown(\"# Final Answer\"))\n",
        "      display(Markdown(ans))\n",
        "      print(cb)\n",
        "\n",
        "      if show_sources:\n",
        "        display(Markdown(\"### Sources\"))\n",
        "        display(Markdown(f\"**[Source ORIGINAL]**\"))\n",
        "        display(Markdown(injected_context))\n",
        "        if injected_meta['category'] != 'paper':\n",
        "          display(Markdown(f\"*File path: {injected_meta['file_path']}*\"))\n",
        "\n",
        "        for i, d in enumerate(context):\n",
        "          display(Markdown(f\"**[Source {i+1}]**\"))\n",
        "          display(Markdown(d.page_content))\n",
        "          if d.metadata['category'] != 'paper':\n",
        "            display(Markdown(f\"*File path: {d.metadata['file_path']}*\"))\n",
        "\n",
        "      return ret\n",
        "\n",
        "\n",
        "# Gets saved in the \"tests/question_answering\" directory\n",
        "def save_qa_pair(file_name, qa_entities):\n",
        "  with open(f\"/content/drive/MyDrive/{file_name}\", 'a+') as file:\n",
        "    for qa in qa_entities:\n",
        "      json.dump(qa, file)\n",
        "      file.write(\"\\n\")\n",
        "\n",
        "\n",
        "# integrated QA pipeline and save on disk on promise, one pipeline is a full chunk document db section (general, technical, whitepaper, etc.)\n",
        "def gen_QA_pairs(chain, texts, metadatas, file_name):\n",
        "  print(f\"total texts: {len(texts)}\")\n",
        "\n",
        "  for i, t in enumerate(texts):\n",
        "    with get_openai_callback() as cb:\n",
        "      print(f\"Working on text #{i+1}\")\n",
        "      qresult = chain.run(document=t)\n",
        "      print(qresult)\n",
        "      q_array = qresult.split('\\n')\n",
        "      print(f\"Asked questions: {q_array}\")\n",
        "      print(cb)\n",
        "\n",
        "      # attempt answer the question in the run freshly with context\n",
        "      qa_array = []\n",
        "      for q in q_array:\n",
        "        qa_object = answer(q, category=metadatas[i]['category'], injected_doc=(t, metadatas[i]), show_sources=False)\n",
        "        qa_array.append(qa_object)\n",
        "\n",
        "      print(f\"\\nSaving QA pairs for text #{i+1}\\n\")\n",
        "      save_qa_pair(file_name, qa_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBqAy3riCX2Q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def complex_answer(query, category, injected_doc, evidences, top_k=5, show_sources=False):\n",
        "    display(Markdown(f\"# Answering query:\\n{query}\"))\n",
        "\n",
        "    llm_chain = LLMChain(\n",
        "      llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
        "      prompt=COMPLEX_ANSWER_PROMPT\n",
        "    )\n",
        "\n",
        "    retriever=vector_db.as_retriever(search_type=\"mmr\", search_kwargs = {\n",
        "      'k': top_k,\n",
        "    })\n",
        "\n",
        "    context = retriever.get_relevant_documents(query)\n",
        "    injected_context, injected_meta = injected_doc\n",
        "\n",
        "    # context is multiple, we store them in the dataset for augmenting knowledge purpose (to inject to llama prompts)\n",
        "    ret = {'question': query, 'answer': \"\", 'category': injected_meta['category'], 'contexts': [injected_context] + [con.page_content for con in context]}\n",
        "\n",
        "    with get_openai_callback() as cb:\n",
        "      ans = llm_chain.run(question=query, context=context, augment_context=injected_context, evidence=evidences)\n",
        "      ret['answer'] = ans\n",
        "      display(Markdown(\"# Final Answer\"))\n",
        "      display(Markdown(ans))\n",
        "      print(cb)\n",
        "\n",
        "      if show_sources:\n",
        "        display(Markdown(\"### Sources\"))\n",
        "        display(Markdown(f\"**[Source ORIGINAL]**\"))\n",
        "        display(Markdown(injected_context))\n",
        "        if injected_meta['category'] != 'paper':\n",
        "          display(Markdown(f\"*File path: {injected_meta['file_path']}*\"))\n",
        "\n",
        "        for i, d in enumerate(context):\n",
        "          display(Markdown(f\"**[Source {i+1}]**\"))\n",
        "          display(Markdown(d.page_content))\n",
        "          if d.metadata['category'] != 'paper':\n",
        "            display(Markdown(f\"*File path: {d.metadata['file_path']}*\"))\n",
        "\n",
        "      return ret\n",
        "\n",
        "def gen_complex_QA_pairs(texts, chain, role, questions_per_text, metadatas, file_name):\n",
        "  print(f\"total texts: {len(texts)}\")\n",
        "\n",
        "  for i, t in enumerate(texts):\n",
        "    with get_openai_callback() as cb:\n",
        "      print(f\"Working on text #{i+1}\")\n",
        "      display(Markdown(t))\n",
        "      qresult = chain.run(document=t, role_description=role_dict[role], k=questions_per_text)\n",
        "      print(qresult)\n",
        "      lines = qresult.split('\\n')\n",
        "\n",
        "      q_array = []\n",
        "      evids = []\n",
        "      for line in lines:\n",
        "        if line.split(':')[0].lower() == 'question':\n",
        "          q_array.append(\"\".join([segment for segment in line.split(':')[1:]]))\n",
        "        elif line.split(':')[0].lower() == 'evidence':\n",
        "          evids.append(\"\".join([segment for segment in line.split(':')[1:]]))\n",
        "\n",
        "      # q_array = json.loads(qresult)['questions']\n",
        "      # evids = json.loads(qresult)['evidences']\n",
        "\n",
        "      for q in q_array:\n",
        "        print(f\"Question asked: {q}\")\n",
        "      for e in evids:\n",
        "        print(f\"Evidences: {e}\")\n",
        "      print(cb)\n",
        "\n",
        "      evid_str = \"\\n\".join([e for e in evids])\n",
        "      qa_array = []\n",
        "      for q in q_array:\n",
        "        qa_object = complex_answer(q, category=metadatas[i]['category'], injected_doc=(t, metadatas[i]), evidences=evid_str, show_sources=False)\n",
        "        qa_array.append(qa_object)\n",
        "\n",
        "      print(f\"\\nSaving QA pairs for text #{i+1}\\n\")\n",
        "      save_qa_pair(file_name, qa_array)\n",
        "\n",
        "def gen_governance_QA_pairs(texts, chain, metadatas, file_name, role='governance', questions_per_text=\"3\"):\n",
        "  with get_openai_callback() as cb:\n",
        "      for i, t in enumerate(texts):\n",
        "        pattern = r\"^# YPP-\\d{4}$\" # YPP-0004, and so on.\n",
        "        header = t.split('\\n')[0]\n",
        "\n",
        "        print(f\"Working on text #{i+1}:{header}...\")\n",
        "        display(Markdown(t))\n",
        "        qresult = chain.run(document=t, role_description=role_dict[role], k=questions_per_text)\n",
        "\n",
        "        # parse the data right here programmatically and not lending this to LLM.\n",
        "        print(qresult)\n",
        "        lines = qresult.split('\\n')\n",
        "\n",
        "        q_array = []\n",
        "        evids = []\n",
        "        for line in lines:\n",
        "          if line.split(':')[0].lower() == 'question':\n",
        "            q_array.append(\"\".join([segment for segment in line.split(':')[1:]]))\n",
        "          elif line.split(':')[0].lower() == 'evidence':\n",
        "            evids.append(\"\".join([segment for segment in line.split(':')[1:]]))\n",
        "\n",
        "        for j in range(len(q_array)):\n",
        "          match_object = re.fullmatch(pattern, header)\n",
        "          if match_object:\n",
        "            q_array[j] = f\"[{header[2:]}] {q_array[j]}\" # make it like [YPP-0004] <question>\n",
        "            print(f\"Question asked: {q_array[j]}\")\n",
        "        for e in evids:\n",
        "          print(f\"Evidences: {e}\")\n",
        "        evid_str = \"\\n\".join([e for e in evids])\n",
        "\n",
        "        qa_array = []\n",
        "        for q in q_array:\n",
        "          qa_object = complex_answer(q, category=metadatas[i]['category'], injected_doc=(t, metadatas[i]), evidences=evid_str, show_sources=False)\n",
        "          qa_array.append(qa_object)\n",
        "\n",
        "        print(f\"\\nSaving QA pairs for text #{i+1}\\n\")\n",
        "        save_qa_pair(file_name, qa_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETnTQ1gEZZTo"
      },
      "outputs": [],
      "source": [
        "# #Uncomment the lines to run functions, or run your own generation partitions.\n",
        "# drive.mount('/content/drive/')\n",
        "# llm=ChatOpenAI(temperature=0.01, model=\"gpt-4\")\n",
        "# complex_question_llm_chain = LLMChain(llm=llm, prompt=COMPLEX_QUESTION_PROMPT)\n",
        "# gen_governance_QA_pairs(governance_splits['all_splits'][501:], complex_question_llm_chain, governance_splits['all_metadatas'][501:], \"governance_long_qa_3.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw5glH_jWPmo"
      },
      "outputs": [],
      "source": [
        "# #Uncomment the lines to run functions, or run your own generation partitions.\n",
        "# drive.mount('/content/drive/')\n",
        "# llm=ChatOpenAI(temperature=0.01, model=\"gpt-4\")\n",
        "# complex_question_llm_chain = LLMChain(llm=llm, prompt=COMPLEX_QUESTION_PROMPT)\n",
        "# gen_complex_QA_pairs(yieldspace_splits['all_splits'][24:27], chain=complex_question_llm_chain, role=\"yieldspace\", questions_per_text=\"3\", metadatas=yieldspace_splits['all_metadatas'][24:27], file_name=\"yieldspace_long_qa.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following sections are no longer in use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJWaFStN2vXq"
      },
      "outputs": [],
      "source": [
        "# # Discarded due to redundancy\n",
        "# governance_proposals_question_prompt_template = \"\"\"\n",
        "# You are a web3 expert using Yield protocol and its services. Here is a governance proposal proposing changes in the protocol.\n",
        "# Write me engaging and profound case study questions rooted in real life web3 user experience scenario using first-person pronoun (I) in the text, based on the provided documents.\n",
        "\n",
        "# # INSTRUCTIONS\n",
        "# - Compose 3 such long, qualitative and engaging real-life case study question that is arisen from the following proposal document\n",
        "# - Always ensure the questions are related to the document text and do not make up any information.\n",
        "# - When you ask a question, make sure that you can also find evidences in the document that makes it answerable.\n",
        "# - The result should be wrapped in JSON format like {{\"questions\": []}}.\n",
        "\n",
        "# # DOCUMENT\n",
        "# {document}\n",
        "\n",
        "# # RESULT\"\"\"\n",
        "\n",
        "# GOV_QUESTION_BANK_PROMPT = PromptTemplate(\n",
        "#     template=governance_proposals_question_prompt_template, input_variables=[\"document\"]\n",
        "# )\n",
        "\n",
        "# governance_question_bank_llm_chain = LLMChain(\n",
        "#     llm=ChatOpenAI(temperature=0.01, model=\"gpt-4\"),\n",
        "#     prompt=GOV_QUESTION_BANK_PROMPT\n",
        "# )\n",
        "# # prompt script for generating long whitepaper questions\n",
        "# # how could it ask questions that can be answered in the\n",
        "# whitepaper_question_prompt_template = \"\"\"\n",
        "# You are a web3 expert using Yield protocol and its services. You are looking at its whitepaper and you want to talk to the authors of this paper to discuss about the details in this paper.\n",
        "# Write me engaging and profound case study questions rooted in real life web3 user experience scenario using first-person pronoun (I) in the text, based on the provided whitepaper pages.\n",
        "\n",
        "# # INSTRUCTIONS\n",
        "# - Compose 4 such long, qualitative and engaging real-life case study question that is arisen from the following whitepaper document.\n",
        "# - Always ensure the questions are related to the document text and do not make up any information.\n",
        "# - When you ask a question, make sure that you can also find evidences in the document that makes it answerable.\n",
        "# - The result should be wrapped in JSON format like {{\"questions\": []}}.\n",
        "\n",
        "# # DOCUMENT\n",
        "# {document}\n",
        "\n",
        "# # RESULT\"\"\"\n",
        "\n",
        "# WP_QUESTION_BANK_PROMPT = PromptTemplate(\n",
        "#     template = whitepaper_question_prompt_template, input_variables=[\"document\"]\n",
        "# )\n",
        "\n",
        "# whitepaper_question_bank_llm_chain = LLMChain(\n",
        "#     llm=ChatOpenAI(temperature=0.01, model=\"gpt-4\"),\n",
        "#     prompt=WP_QUESTION_BANK_PROMPT\n",
        "# )\n",
        "# yieldspace_question_prompt_template = \"\"\"\n",
        "# You are a web3 expert using Yield protocol and its services. You are looking at its newly published paper called 'yield space' and you want to talk to the authors of this paper to discuss about the details in this paper.\n",
        "# Especially you noticed that this paper is heavily focused on mathematical formulas related to the concept of 'invariants' in decentralized finance.\n",
        "# You are very interested in studying the maths behind this paper, pay special attention to the numeric relationships between objects in this passage.\n",
        "# Write me engaging and profound case study questions rooted in web3 finance researcher experience using first-person pronoun (I) in the text, based on the provided paper pages.\n",
        "\n",
        "# # INSTRUCTIONS\n",
        "# - Compose 4 such long, qualitative and engaging real-life case study question that is arisen from the following whitepaper document.\n",
        "# - Always ensure the questions are related to the document text and do not make up any information.\n",
        "# - When you ask a question, make sure that you can also find evidences in the document that makes it answerable.\n",
        "# - The result should be wrapped in JSON format like {{\"questions\": []}}.\n",
        "\n",
        "# # DOCUMENT\n",
        "# {document}\n",
        "\n",
        "# # RESULT\"\"\"\n",
        "\n",
        "# YS_QUESTION_BANK_PROMPT = PromptTemplate(\n",
        "#     template = yieldspace_question_prompt_template, input_variables=[\"document\"]\n",
        "# )\n",
        "\n",
        "# yieldspace_question_bank_llm_chain = LLMChain(\n",
        "#     llm=ChatOpenAI(temperature=0.01, model=\"gpt-4\"),\n",
        "#     prompt=YS_QUESTION_BANK_PROMPT\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
