{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LLaMA 2 on Amazon SageMaker\n",
    "\n",
    "In this sagemaker example, we are going to learn how to fine-tune [LLaMA 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) using [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314). [LLaMA 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) is the next version of the [LLaMA](https://arxiv.org/abs/2302.13971). Compared to the V1 model, it is trained on more data - 2T tokens and supports context length window upto 4K tokens. Learn more about LLaMa 2 in the [\"\"]() blog post.\n",
    "\n",
    "QLoRA is an efficient finetuning technique that quantizes a pretrained language model to 4 bits and attaches small “Low-Rank Adapters” which are fine-tuned. This enables fine-tuning of models with up to 65 billion parameters on a single GPU; despite its efficiency, QLoRA matches the performance of full-precision fine-tuning and achieves state-of-the-art results on language tasks.\n",
    "\n",
    "In our example, we are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft). \n",
    "\n",
    "In Detail you will learn how to:\n",
    "1. Setup Development Environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-Tune LLaMA 13B with QLoRA on Amazon SageMaker\n",
    "4. Deploy Fine-tuned LLM on Amazon SageMaker\n",
    "\n",
    "### Quick intro: PEFT or Parameter Efficient Fine-tuning\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n",
    "\n",
    "- (Q)LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning: [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "- IA3: [Infused Adapter by Inhibiting and Amplifying Inner Activations](https://arxiv.org/abs/2205.05638)\n",
    "\n",
    "\n",
    "\n",
    "### Access LLaMA 2\n",
    "\n",
    "Before we can start training we have to make sure that we accepted the license of [llama 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) to be able to use it. You can accept the license by clicking on the Agree and access repository button on the model page at: \n",
    "* [LLaMa 7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n",
    "* [LLaMa 13B](https://huggingface.co/meta-llama/Llama-2-13b-hf)\n",
    "* [LLaMa 70B](https://huggingface.co/meta-llama/Llama-2-70b-hf)\n",
    "\n",
    "## 1. Setup Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access any LLaMA 2 asset we need to login into our hugging face account. We can do this by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "data_files = {\"train\": \"qa_pairs_train.jsonl\", \"test\": \"qa_pairs_test.jsonl\"}\n",
    "dataset = load_dataset(\"YieldInc/chatbot_qa_dataset_splitted\", data_files=data_files)['train']\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_yield(sample):\n",
    "    instruction = f\"%%% Instruction\\n{sample['question']}\"\n",
    "    context = f\"%%% Context\\n{sample['contexts'][0]}\\n{sample['contexts'][1]}\\n\"\n",
    "    response = f\"%%% Answer\\n{sample['answer']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%% Instruction\n",
      "[YPP-0051]  As an investor in the Yield protocol, how would the updating of the 0x303030370000 series with new ilks impact my investment? What does the use of a specific pool for this series mean for the liquidity and value of my tokens?\n",
      "\n",
      "%%% Context\n",
      "# YPP-0051\n",
      "\n",
      "fyToken.revokeRole(ROOT, deployer)\n",
      "fyToken.grantRole(ROOT, cloak)\n",
      "cloak.plan(ladle, fyToken(0007)): 0x12eeabf06a3ae55ce160a4280fdb10c130e74e52c2839479355292704c2081d7\n",
      "cloak.plan(fyToken, join(00)): 0x8362c03c1646112befa3e5f823d9e5a22668643a64782a91f849ba89ca9760ac\n",
      "Updating 0x303030370000 series with 0x303000000000,0x303100000000,0x303200000000 ilks\n",
      "addIlks 0007: 0x303000000000,0x303100000000,0x303200000000\n",
      "Using pool at 0x3Aa5ebB10DC797CAC828524e59A333d0A371443c for 0x303030370000\n",
      "Timelock balance of 0x303000000000 is 80000000000000000\n",
      "Transferring 20000000000000000 of 0x303000000000 from Timelock to Pool\n",
      "Initializing FYETH2209LP at 0x3Aa5ebB10DC797CAC828524e59A333d0A371443c\n",
      "Transferring 0 of 0x303000000000 from Timelock to Join\n",
      "Minting 0 amount with underlying to 0x3Aa5ebB10DC797CAC828524e59A333d0A371443c\n",
      "strategy(YSETH6MMS).grantRoles(gov, timelock)\n",
      "strategy(YSETH6MMS).revokeRole(ROOT, deployer)\n",
      "Timelock balance of Wrapped Ether is 80000000000000000\n",
      "Setting 0x3Aa5ebB10DC797CAC828524e59A333d0A371443c as the next pool for YSETH6MMS\n",
      "Transferring 20000000000000000 of 0x82aF49447D8a07e3bd95BD0d56f35241523fBab1 to 0xc6e7DF5E7b4f2A278906862b61205850344D4e7d\n",
      "Starting YSETH6MMS at 0xc6e7DF5E7b4f2A278906862b61205850344D4e7d\n",
      "Burning strategy tokens\n",
      "Setting YSETH6MMS as an integration in the Ladle\n",
      "Setting YSETH6MMS as a token in the Ladle\n",
      "Proposal: 0x09af7c12e664c93f4967150bbf1bda0acccf6eb0d0ed5cea431b652390b62b49\n",
      "Proposing\n",
      "Impersonated 0xC7aE076086623ecEA2450e364C838916a043F9a8\n",
      "Developer: 0xC7aE076086623ecEA2450e364C838916a043F9a8\n",
      "Proposed 0x09af7c12e664c93f4967150bbf1bda0acccf6eb0d0ed5cea431b652390b62b49\n",
      "+ npx hardhat run --network localhost ./scripts/governance/add/addSeries/addEthSeries/arbitrum/addEthSeries.ts\n",
      "No need to generate any newer typings.\n",
      "Impersonating 0xC7aE076086623ecEA2450e364C838916a043F9a8 on localhost\n",
      "compoundOracle: 0x0ad9Ef93673B6081c0c3b753CcaaBDdd8d2e7848\n",
      "Accumulator(00/0x524154450000): 1000000000000000000, 1000000000000000000\n",
      "# YPP-0032\n",
      "\n",
      "Updating 0x303130370000 series with 0x303000000000,0x303100000000,0x303200000000 ilks\n",
      "addIlks 0107: 0x303000000000,0x303100000000,0x303200000000\n",
      "Updating 0x303230370000 series with 0x303000000000,0x303100000000,0x303200000000 ilks\n",
      "addIlks 0207: 0x303000000000,0x303100000000,0x303200000000\n",
      "Using pool at 0x610178dA211FEF7D417bC0e6FeD39F05609AD788 for 0x303130370000\n",
      "Timelock balance of 0x303100000000 is 101414980801618940905\n",
      "Transferring 100000000000000000000 of 0x303100000000 from Timelock to Pool\n",
      "Initializing FYDAI2209LP at 0x610178dA211FEF7D417bC0e6FeD39F05609AD788\n",
      "Transferring 0 of 0x303100000000 from Timelock to Join\n",
      "Minting 0 amount with underlying to 0x610178dA211FEF7D417bC0e6FeD39F05609AD788\n",
      "Using pool at 0xB7f8BC63BbcaD18155201308C8f3540b07f84F5e for 0x303230370000\n",
      "Timelock balance of 0x303200000000 is 101940880\n",
      "Transferring 100000000 of 0x303200000000 from Timelock to Pool\n",
      "Initializing FYUSDC2209LP at 0xB7f8BC63BbcaD18155201308C8f3540b07f84F5e\n",
      "Transferring 0 of 0x303200000000 from Timelock to Join\n",
      "Minting 0 amount with underlying to 0xB7f8BC63BbcaD18155201308C8f3540b07f84F5e\n",
      "Using strategy at 0xE779cd75E6c574d83D3FD6C92F3CBE31DD32B1E1 for YSDAI6MMS\n",
      "Using pool at 0x610178dA211FEF7D417bC0e6FeD39F05609AD788 for 0x303130370000\n",
      "Strategy YSDAI6MMS divested from 0x303130350000\n",
      "Strategy YSDAI6MMS rolled onto 0x303130370000\n",
      "Using strategy at 0x92A5B31310a3ED4546e0541197a32101fCfBD5c8 for YSUSDC6MMS\n",
      "Using pool at 0xB7f8BC63BbcaD18155201308C8f3540b07f84F5e for 0x303230370000\n",
      "Strategy YSUSDC6MMS divested from 0x303230350000\n",
      "Strategy YSUSDC6MMS rolled onto 0x303230370000\n",
      "Proposal: 0x52466d6952dc0513a3d13ff7b2a9f13a8626239c762ff738f3d303f6f8aacca2\n",
      "Executing\n",
      "Executed 0x52466d6952dc0513a3d13ff7b2a9f13a8626239c762ff738f3d303f6f8aacca2\n",
      "\n",
      "\n",
      "%%% Answer\n",
      "The updating of the 0x303030370000 series with new ilks (0x303000000000,0x303100000000,0x303200000000) in the Yield Protocol is part of the process of managing the liquidity of the fyTokens associated with that series. An ilk in this context refers to a type of collateral that can be used in the Yield Protocol.\n",
      "\n",
      "The use of a specific pool (0x3Aa5ebB10DC797CAC828524e59A333d0A371443c in this case) for the 0x303030370000 series means that the liquidity for this series is concentrated in this pool. This can have several implications for your investment:\n",
      "\n",
      "1. **Liquidity**: The liquidity of your tokens (fyTokens) is directly related to the liquidity of the pool. A well-funded pool means that there will be enough liquidity for you to trade your tokens when you want to.\n",
      "\n",
      "2. **Value**: The value of your tokens can be influenced by the activities in the pool. For instance, a large amount of trading activity in the pool can potentially impact the price of the tokens.\n",
      "\n",
      "3. **Risk**: Concentrating liquidity in a single pool can also mean that the risk is concentrated. If something adverse happens to the pool (e.g., a smart contract bug), it could potentially impact all the tokens associated with that pool.\n",
      "\n",
      "4. **Fees**: As a liquidity provider, you can earn fees from the trading activity that happens in the pool. The more activity there is, the more fees you can potentially earn.\n",
      "\n",
      "Remember that while providing liquidity can earn you fees, it also comes with risks such as impermanent loss. It's important to understand these risks before you decide to provide liquidity.\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_yield(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-13b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': Value(dtype='string', id=None),\n",
       " 'answer': Value(dtype='string', id=None),\n",
       " 'category': Value(dtype='string', id=None),\n",
       " 'contexts': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some helper functions to pack our samples into sequences of a given length and then tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3487 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%% Instruction\n",
      "I am using the Yield Protocol and I am curious about the AccumulatorMultiOracle's get function. It computes the return value and stores it in the `accumulated` field, updating the lastUpdated timestamp. How does this process ensure the efficiency of the protocol, and what would be the potential impact if this step was not implemented?\n",
      "\n",
      "%%% Context\n",
      "# AccumulatorMultiOracle\n",
      "\n",
      "### get\n",
      "\n",
      "```solidity\n",
      "function get(bytes32 base, bytes32 kind, uint256) external virtual returns (uint256 accumulated, uint256 updateTime)\n",
      "```\n",
      "\n",
      "Retrieve the latest accumulated rate from source, updating it if necessary.\n",
      "\n",
      "    Computes baseRate ^ (block.timestamp - creation timestamp)\n",
      "\n",
      "    pow() is not O(1), so the naive implementation will become slower as the time passes\n",
      "    To workaround that, each time get() is called, we:\n",
      "        1) compute the return value\n",
      "        2) store the return value in `accumulated` field, update lastUpdated timestamp\n",
      "\n",
      "    Becase we have `accumulated`, step 1 becomes `accumulated * baseRate ^ (block.timestamp - lastUpdated)\n",
      "build a perpetual product on top of it, by implementing a pool that invests in short-term fyTokens and automatically rolls them over upon expiry.\n",
      "\n",
      "Perhaps most interestingly, the market prices of fyTokens can be used as an “interest rate oracle.” The price of each maturity of fyToken implies a partic- ular yield. These implied yields could be used to settle on-chain interest rate derivatives, to inform Maker’s choice of stability fee, or as an input to the in- terest rate formulas used by Compound or dYdX. By charting those yields for diﬀerent maturities, we can even construct a yield curve, which could be a useful indicator of the expected path of interest rates or prices.\n",
      "\n",
      "This paper will present three constructions of fyTokens. The ﬁrst (“cash settlement”) assumes the existence of a price oracle that, when asked, can tell the contract the value of the target asset in terms of the collateral asset. The second (“physical settlement”) does not require any price oracle, but assumes that the target asset is itself a token (rather than an oﬀ-chain asset like USD). The third (settlement to synthetics) assumes that the target asset is either a collateralized synthetic like Maker, or supported by a ﬂoating-rate lending platform like Compound.\n",
      "\n",
      "2 Prior work\n",
      "\n",
      "The design of the Yield Protocol is heavily inﬂuenced by other projects on the Ethereum blockchain that oﬀer synthetics, borrowing, lending, and/or leverage. Yield primarily diﬀers in that its interest rates are implicit and set by market prices, rather than being set by governance or a formula. Additionally, whereas most lending protocols use ﬂoating interest rates, Yield enables term loans with ﬁxed interest rates, while still maintaining some degree of fungibility. Section 4.3 shows how, thanks to these properties, the Yield Protocol can provide unique insight into the term structure of interest rates for on-chain lending.\n",
      "\n",
      "\n",
      "%%% Answer\n",
      "The `get` function in the AccumulatorMultiOracle contract is designed to efficiently compute the accumulated rate from a source, updating it if necessary. The process involves computing `baseRate ^ (block.timestamp - creation timestamp)`. However, the `pow()` function is not O(1), meaning that the naive implementation will become slower as time passes.\n",
      "\n",
      "To ensure efficiency, each time `get()` is called, the function does two things:\n",
      "1) It computes the return value.\n",
      "2) It stores the return value in the `accumulated` field and updates the `lastUpdated` timestamp.\n",
      "\n",
      "Because the `accumulated` field is updated each time `get()` is called, the computation in step 1 becomes `accumulated * baseRate ^ (block.timestamp - lastUpdated)`. This approach ensures that the computation remains efficient even as time passes, as it avoids the need to compute the power of the base rate and the difference between the current timestamp and the creation timestamp directly.\n",
      "\n",
      "If this step was not implemented, the computation of the accumulated rate would become slower over time, as it would involve computing the power of the base rate and the difference between the current timestamp and the creation timestamp directly each time. This could potentially slow down the protocol, especially if the `get()` function is called frequently.</s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3487 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3487 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 2079\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_yield(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize the dataset and append data\n",
    "def transform(sample, max_token=2048):\n",
    "    result = tokenizer(sample[\"text\"], padding='max_length', truncation=True, max_length=max_token)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# concatenating different samples is totally fine and not disintegrating coherence between the original samples if adding delimiting token <s> and </s>.\n",
    "# this method saves time and resource to not computing wasteful paddings from the attention masks.\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 1273, 7686, 2799, 4080, 13, 13555, 7246, 292, 263, 716, 5988, 773, 278, 1653, 17242, 740, 29892, 825, 674, 306, 29892, 408, 263, 1404, 29892, 7150, 297, 736, 322, 920, 508, 306, 671, 445, 2472, 297, 278, 3030, 310, 612, 969, 1019, 5770, 29973, 13, 13, 7686, 29995, 15228, 13, 29937, 3650, 262, 5126, 13, 13, 1576, 3650, 262, 5126, 10017, 716, 5988, 8871, 29889, 13, 13, 2277, 6680, 29879, 13, 2277, 29937, 1653, 17242, 13, 28956, 2929, 333, 537, 13, 29871, 740, 1653, 17242, 29898, 13, 1678, 3211, 24342, 13, 29871, 1723, 7029, 3639, 313, 7328, 29897, 13, 28956, 13, 13, 8498, 417, 952, 263, 716, 5988, 29889, 13, 13, 4136, 12662, 2699, 29901, 13, 29989, 4408, 891, 5167, 891, 12953, 462, 462, 462, 3986, 891, 13, 29989, 584, 5634, 891, 584, 5634, 891, 584, 2683, 2683, 2683, 2683, 5634, 891, 13, 29989, 29952, 24129, 29952, 891, 3211, 891, 16428, 310, 278, 24342, 5993, 29889, 13, 13, 4136, 7106, 2630, 1041, 29901, 13, 29989, 4408, 462, 965, 891, 5167, 3986, 891, 12953, 462, 462, 462, 462, 29871, 891, 13, 29989, 584, 2683, 9072, 29899, 891, 584, 9072, 891, 584, 2683, 2683, 2683, 2683, 1378, 5634, 891, 13, 29989, 29952, 7122, 29952, 29989, 3211, 891, 450, 5988, 3211, 29889, 13, 29937, 5195, 3035, 2303, 13, 13, 29937, 10682, 261, 10854, 362, 13, 13, 1576, 612, 969, 1019, 5770, 338, 13931, 964, 1023, 28914, 29892, 697, 363, 278, 5321, 1008, 284, 1891, 2553, 29873, 6012, 2000, 325, 1292, 29879, 322, 697, 363, 278, 3345, 630, 9999, 2136, 261, 2000, 612, 969, 14936, 29889, 13, 13, 4178, 278, 7136, 310, 612, 969, 526, 1404, 29899, 26689, 30081, 1068, 29963, 1292, 29879, 1068, 30081, 276, 6338, 292, 263, 5321, 1008, 284, 1891, 2553, 29873, 2602, 29889, 7806, 325, 1292, 338, 6942, 411, 2323, 5321, 1008, 284, 322, 2553, 29873, 297, 263, 2323, 3652, 29889, 319, 3652, 11524, 263, 2323, 27942, 519, 24342, 411, 263, 3342, 286, 1337, 537, 2635, 29889, 1152, 1342, 29892, 16308, 1122, 1914, 263, 325, 1292, 411, 382, 4690, 5321, 1008, 284, 322, 2553, 29873, 297, 278, 3148, 12696, 29900, 29929, 29906, 29945, 3652, 29889, 450, 29871, 3148, 12696, 29900, 29929, 29906, 29945, 3652, 11524, 385, 10788, 362, 304, 1634, 388, 3148, 12696, 373, 3839, 29871, 29906, 29945, 386, 29892, 29871, 29906, 29900, 29906, 29896, 29889, 13, 13, 29963, 1292, 29879, 526, 8745, 491, 278, 30081, 1068, 26270, 16722, 1617, 1068, 29892, 263, 15040, 8078, 393, 6475, 278, 5321, 1008, 284, 322, 2553, 29873, 363, 1269, 325, 1292, 29889, 450, 9243, 16722, 1617, 3635, 1169, 10643, 310, 278, 2989, 11747, 17437, 310, 263, 325, 1292, 29892, 515, 4969, 263, 325, 1292, 29892, 4417, 322, 11077, 5321, 1008, 284, 29892, 4417, 322, 11077, 2553, 29873, 29892, 8454, 5321, 1008, 284, 2133, 29892, 3635, 5367, 23904, 362, 310, 1090, 22017, 1008, 284, 1891, 325, 1292, 29879, 29892, 322, 27777, 5321, 1008, 284, 322, 2553, 29873, 304, 263, 716, 3652, 29889, 13, 13, 10251, 29873, 297, 612, 969, 338, 5993, 1891, 297, 278, 883, 310, 285, 29891, 29911, 554, 575, 313, 30015, 20227, 7709, 18897, 30024, 467, 285, 29891, 29911, 554, 575, 526, 382, 12711, 398, 2729, 8982, 29907, 29899, 29906, 29900, 18897, 393, 508, 367, 337, 311, 22580, 363, 385, 14407, 24342, 697, 29899, 517, 29899, 650, 1156, 263, 4450, 300, 837, 1312, 286, 1337, 537, 2635, 29889, 1152, 1342, 29892, 285, 29891, 3308, 12696, 29900, 29929, 29906, 29945, 18897, 526, 337, 311, 331, 519, 363, 3148, 12696, 1156, 3839, 29871, 29906, 29945, 386, 29892, 29871, 29906, 29900, 29906, 29896, 29889, 7806, 285, 29891, 6066, 286, 1337, 537, 756, 385, 6942, 30081, 1068, 29943, 29891, 6066, 1068, 30081, 1001, 29907, 29899, 29906, 29900, 15040, 8078, 29889, 13, 13, 9760, 1134, 310, 5321, 1008, 284, 297, 612, 969, 1122, 367, 409, 1119, 14561, 964, 263, 13587, 8078, 470, 30081, 1068, 17242, 1068, 30081, 1454, 393, 5321, 1008, 284, 29889, 3650, 262, 8078, 29879, 7150, 21608, 515, 4160, 322, 3787, 372, 2745, 372, 338, 6206, 515, 278, 1788, 29889, 3650, 262, 947, 451, 2407, 738, 325, 1292, 2106, 29889, 3650, 262, 3635, 1169, 4160, 304, 11013, 27942, 599, 21608, 372, 8640, 29889, 13, 13, 13, 7686, 29995, 673, 13, 10401, 366, 7246, 263, 716, 5988, 773, 278, 421, 3258, 17242, 29952, 740, 29892, 366, 674, 7150, 278, 3211, 310, 278, 15141, 2825, 5988, 8078, 29889, 910, 338, 9875, 491, 278, 421, 7122, 29952, 736, 995, 297, 278, 740, 29889, 13, 13, 28956, 2929, 333, 537, 13, 29871, 740, 1653, 17242, 29898, 13, 1678, 3211, 24342, 13, 29871, 1723, 7029, 3639, 313, 7328, 5988, 29897, 13, 28956, 13, 13, 797, 278, 3030, 310, 612, 969, 1019, 5770, 29892, 278, 5988, 3211, 338, 1407, 4100, 29889, 7806, 1134, 310, 5321, 1008, 284, 297, 612, 969, 1122, 367, 409, 1119, 14561, 964, 263, 13587, 8078, 470, 3579, 17242, 1068, 363, 393, 5321, 1008, 284, 29889, 3650, 262, 8078, 29879, 7150, 21608, 515, 4160, 322, 3787, 372, 2745, 372, 338, 6206, 515, 278, 1788, 29889, 29871, 13, 13, 8439, 1079, 29892, 278, 5988, 3211, 366, 7150, 508, 367, 1304, 304, 16254, 411, 278, 5988, 8078, 29889, 1152, 1342, 29892, 366, 508, 19754, 277, 21608, 964, 278, 5988, 8078, 29892, 470, 28679, 21608, 515, 372, 29889, 29871, 13, 13, 7301, 1096, 393, 278, 3650, 262, 8078, 947, 451, 2407, 738, 325, 1292, 2106, 29889, 739, 3763, 8640, 278, 21608, 2745, 896, 526, 6206, 515, 278, 1788, 29889, 450, 10643, 310, 278, 2989, 11747, 17437, 310, 263, 325, 1292, 29892, 515, 4969, 263, 325, 1292, 29892, 4417, 322, 11077, 5321, 1008, 284, 29892, 4417, 322, 11077, 2553, 29873, 29892, 8454, 5321, 1008, 284, 2133, 29892, 3635, 5367, 23904, 362, 310, 1090, 22017, 1008, 284, 1891, 325, 1292, 29879, 29892, 322, 27777, 5321, 1008, 284, 322, 2553, 29873, 304, 263, 716, 3652, 29892, 338, 2309, 491, 278, 3579, 26270, 16722, 1617, 1068, 8078, 29889, 29871, 13, 13, 6295, 29892, 297, 15837, 29892, 278, 5988, 3211, 366, 7150, 515, 278, 421, 3258, 17242, 29952, 740, 338, 278, 3211, 310, 278, 8078, 988, 366, 508, 19754, 277, 470, 28679, 21608, 29889, 2, 1, 1273, 7686, 2799, 4080, 13, 5618, 947, 278, 19392, 29886, 473, 29952, 740, 437, 29973, 13, 13, 7686, 29995, 15228, 13, 29937, 9243, 16722, 1617, 13, 13, 2277, 29937, 903, 29886, 473, 13, 13, 28956, 2929, 333, 537, 13, 2220, 903, 29886, 473, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 2281, 3630, 10562, 29889, 29963, 1292, 325, 1292, 3383, 2281, 3630, 10562, 29889, 22031, 2925, 6411, 2925, 3383, 2281, 3630, 10562, 29889, 19204, 3652, 3383, 938, 29896, 29906, 29947, 297, 29895, 29892, 938, 29896, 29906, 29947, 1616, 29897, 7463, 3639, 313, 4984, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 2528, 5321, 1008, 284, 322, 27942, 515, 325, 1292, 29892, 8206, 21608, 515, 322, 5503, 27942, 287, 24342, 304, 1404, 13, 2816, 29892, 1634, 388, 304, 325, 1292, 322, 3349, 5321, 1008, 284, 29892, 8206, 27942, 287, 24342, 515, 322, 5503, 21608, 304, 1404, 29918, 13, 13, 2277, 29937, 1671, 13, 13, 28956, 2929, 333, 537, 13, 2220, 1671, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 938, 29896, 29906, 29947, 297, 29895, 29892, 938, 29896, 29906, 29947, 1616, 29897, 7029, 6901, 3639, 313, 4984, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 2517, 666, 5987, 263, 325, 1292, 29892, 5662, 3864, 372, 338, 5321, 1008, 284, 1891, 12335, 29889, 13, 1762, 367, 1304, 491, 2553, 29873, 10643, 8078, 29879, 3032, 13, 13, 2277, 29937, 2243, 332, 29886, 13, 13, 28956, 2929, 333, 537, 13, 2220, 2243, 332, 29886, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 13122, 29896, 29906, 29947, 297, 29895, 29892, 13122, 29896, 29906, 29947, 1616, 29897, 7029, 3639, 313, 4984, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 29934, 6085, 346, 2553, 29873, 322, 5321, 1008, 284, 515, 263, 325, 1292, 29892, 5330, 8253, 5321, 1008, 284, 2133, 12747, 29889, 13, 1762, 367, 1304, 491, 23904, 362, 24000, 3032, 13, 13, 2277, 29937, 9679, 13, 13, 28956, 2929, 333, 537, 13, 2220, 9679, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 6262, 29953, 716, 19204, 1204, 29892, 938, 29896, 29906, 29947, 1616, 29897, 7029, 3639, 313, 4984, 3630, 10562, 29889, 29963, 1292, 29892, 2281, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 7277, 3652, 322, 2553, 29873, 310, 263, 325, 1292, 29889, 13, 1576, 3883, 5432, 445, 740, 884, 4225, 304, 15649, 14407, 297, 278, 11565, 363, 278, 716, 3652, 29892, 322, 19417, 372, 297, 11565, 363, 278, 2030, 3652, 3032, 13, 13, 2277, 29937, 3233, 13, 13, 28956, 2929, 333, 537, 13, 2220, 3233, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29897, 7029, 3639, 313, 524, 29906, 29945, 29953, 29897, 13, 28956, 13, 13, 29918, 11609, 278, 5321, 1008, 284, 2133, 3233, 310, 263, 325, 1292, 29889, 739, 674, 367, 8178, 565, 1090, 22017, 1008, 284, 1891, 3032, 13, 13, 2277, 29937, 286, 1535, 13, 13, 28956, 2929, 333, 537, 13, 2220, 286, 1535, 29898, 13193, 29953, 3652, 1204, 29897, 7029, 13, 28956, 13, 13, 29918, 9182, 278, 27942, 292, 6554, 472, 286, 1337, 537, 363, 263, 3652, 29918, 13, 13, 2277, 29937, 903, 29885, 1535, 13, 13, 28956, 2929, 333, 537, 13, 2220, 903, 29885, 1535, 29898, 13193, 29953, 3652, 1204, 29892, 2281, 3630, 10562, 29889, 19204, 3652, 19925, 7463, 13, 28956, 13, 13, 29918, 9182, 278, 27942, 292, 6554, 472, 286, 1337, 537, 363, 263, 3652, 29918, 13, 13, 2277, 29937, 1035, 582, 284, 13, 13, 28956, 2929, 333, 537, 13, 2220, 1035, 582, 284, 29898, 13193, 29953, 3652, 1204, 29897, 7029, 3639, 313, 13470, 29906, 29945, 29953, 29897, 13, 28956, 13, 13, 29918, 8015, 29878, 2418, 278, 6554, 1035, 582, 284, 1951, 286, 1337, 537, 29892, 286, 19021, 565, 5181, 3032, 13, 29937, 9243, 16722, 1617, 13, 13, 2277, 29937, 903, 29886, 473, 13, 13, 28956, 2929, 333, 537, 13, 2220, 903, 29886, 473, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 2281, 3630, 10562, 29889, 29963, 1292, 325, 1292, 3383, 2281, 3630, 10562, 29889, 22031, 2925, 6411, 2925, 3383, 2281, 3630, 10562, 29889, 19204, 3652, 3383, 938, 29896, 29906, 29947, 297, 29895, 29892, 938, 29896, 29906, 29947, 1616, 29897, 7463, 3639, 313, 4984, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 2528, 5321, 1008, 284, 322, 27942, 515, 325, 1292, 29892, 8206, 21608, 515, 322, 5503, 27942, 287, 24342, 304, 1404, 13, 2816, 29892, 1634, 388, 304, 325, 1292, 322, 3349, 5321, 1008, 284, 29892, 8206, 27942, 287, 24342, 515, 322, 5503, 21608, 304, 1404, 29918, 13, 13, 2277, 29937, 1671, 13, 13, 28956, 2929, 333, 537, 13, 2220, 1671, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 938, 29896, 29906, 29947, 297, 29895, 29892, 938, 29896, 29906, 29947, 1616, 29897, 7029, 6901, 3639, 313, 4984, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 2517, 666, 5987, 263, 325, 1292, 29892, 5662, 3864, 372, 338, 5321, 1008, 284, 1891, 12335, 29889, 13, 1762, 367, 1304, 491, 2553, 29873, 10643, 8078, 29879, 3032, 13, 13, 2277, 29937, 2243, 332, 29886, 13, 13, 28956, 2929, 333, 537, 13, 2220, 2243, 332, 29886, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 13122, 29896, 29906, 29947, 297, 29895, 29892, 13122, 29896, 29906, 29947, 1616, 29897, 7029, 3639, 313, 4984, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 29934, 6085, 346, 2553, 29873, 322, 5321, 1008, 284, 515, 263, 325, 1292, 29892, 5330, 8253, 5321, 1008, 284, 2133, 12747, 29889, 13, 1762, 367, 1304, 491, 23904, 362, 24000, 3032, 13, 13, 2277, 29937, 9679, 13, 13, 28956, 2929, 333, 537, 13, 2220, 9679, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 6262, 29953, 716, 19204, 1204, 29892, 938, 29896, 29906, 29947, 1616, 29897, 7029, 3639, 313, 4984, 3630, 10562, 29889, 29963, 1292, 29892, 2281, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 7277, 3652, 322, 2553, 29873, 310, 263, 325, 1292, 29889, 13, 1576, 3883, 5432, 445, 740, 884, 4225, 304, 15649, 14407, 297, 278, 11565, 363, 278, 716, 3652, 29892, 322, 19417, 372, 297, 11565, 363, 278, 2030, 3652, 3032, 13, 13, 2277, 29937, 3233, 13, 13, 28956, 2929, 333, 537, 13, 2220, 3233, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29897, 7029, 3639, 313, 524, 29906, 29945, 29953, 29897, 13, 28956, 13, 13, 29918, 11609, 278], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 1273, 7686, 2799, 4080, 13, 13555, 7246, 292, 263, 716, 5988, 773, 278, 1653, 17242, 740, 29892, 825, 674, 306, 29892, 408, 263, 1404, 29892, 7150, 297, 736, 322, 920, 508, 306, 671, 445, 2472, 297, 278, 3030, 310, 612, 969, 1019, 5770, 29973, 13, 13, 7686, 29995, 15228, 13, 29937, 3650, 262, 5126, 13, 13, 1576, 3650, 262, 5126, 10017, 716, 5988, 8871, 29889, 13, 13, 2277, 6680, 29879, 13, 2277, 29937, 1653, 17242, 13, 28956, 2929, 333, 537, 13, 29871, 740, 1653, 17242, 29898, 13, 1678, 3211, 24342, 13, 29871, 1723, 7029, 3639, 313, 7328, 29897, 13, 28956, 13, 13, 8498, 417, 952, 263, 716, 5988, 29889, 13, 13, 4136, 12662, 2699, 29901, 13, 29989, 4408, 891, 5167, 891, 12953, 462, 462, 462, 3986, 891, 13, 29989, 584, 5634, 891, 584, 5634, 891, 584, 2683, 2683, 2683, 2683, 5634, 891, 13, 29989, 29952, 24129, 29952, 891, 3211, 891, 16428, 310, 278, 24342, 5993, 29889, 13, 13, 4136, 7106, 2630, 1041, 29901, 13, 29989, 4408, 462, 965, 891, 5167, 3986, 891, 12953, 462, 462, 462, 462, 29871, 891, 13, 29989, 584, 2683, 9072, 29899, 891, 584, 9072, 891, 584, 2683, 2683, 2683, 2683, 1378, 5634, 891, 13, 29989, 29952, 7122, 29952, 29989, 3211, 891, 450, 5988, 3211, 29889, 13, 29937, 5195, 3035, 2303, 13, 13, 29937, 10682, 261, 10854, 362, 13, 13, 1576, 612, 969, 1019, 5770, 338, 13931, 964, 1023, 28914, 29892, 697, 363, 278, 5321, 1008, 284, 1891, 2553, 29873, 6012, 2000, 325, 1292, 29879, 322, 697, 363, 278, 3345, 630, 9999, 2136, 261, 2000, 612, 969, 14936, 29889, 13, 13, 4178, 278, 7136, 310, 612, 969, 526, 1404, 29899, 26689, 30081, 1068, 29963, 1292, 29879, 1068, 30081, 276, 6338, 292, 263, 5321, 1008, 284, 1891, 2553, 29873, 2602, 29889, 7806, 325, 1292, 338, 6942, 411, 2323, 5321, 1008, 284, 322, 2553, 29873, 297, 263, 2323, 3652, 29889, 319, 3652, 11524, 263, 2323, 27942, 519, 24342, 411, 263, 3342, 286, 1337, 537, 2635, 29889, 1152, 1342, 29892, 16308, 1122, 1914, 263, 325, 1292, 411, 382, 4690, 5321, 1008, 284, 322, 2553, 29873, 297, 278, 3148, 12696, 29900, 29929, 29906, 29945, 3652, 29889, 450, 29871, 3148, 12696, 29900, 29929, 29906, 29945, 3652, 11524, 385, 10788, 362, 304, 1634, 388, 3148, 12696, 373, 3839, 29871, 29906, 29945, 386, 29892, 29871, 29906, 29900, 29906, 29896, 29889, 13, 13, 29963, 1292, 29879, 526, 8745, 491, 278, 30081, 1068, 26270, 16722, 1617, 1068, 29892, 263, 15040, 8078, 393, 6475, 278, 5321, 1008, 284, 322, 2553, 29873, 363, 1269, 325, 1292, 29889, 450, 9243, 16722, 1617, 3635, 1169, 10643, 310, 278, 2989, 11747, 17437, 310, 263, 325, 1292, 29892, 515, 4969, 263, 325, 1292, 29892, 4417, 322, 11077, 5321, 1008, 284, 29892, 4417, 322, 11077, 2553, 29873, 29892, 8454, 5321, 1008, 284, 2133, 29892, 3635, 5367, 23904, 362, 310, 1090, 22017, 1008, 284, 1891, 325, 1292, 29879, 29892, 322, 27777, 5321, 1008, 284, 322, 2553, 29873, 304, 263, 716, 3652, 29889, 13, 13, 10251, 29873, 297, 612, 969, 338, 5993, 1891, 297, 278, 883, 310, 285, 29891, 29911, 554, 575, 313, 30015, 20227, 7709, 18897, 30024, 467, 285, 29891, 29911, 554, 575, 526, 382, 12711, 398, 2729, 8982, 29907, 29899, 29906, 29900, 18897, 393, 508, 367, 337, 311, 22580, 363, 385, 14407, 24342, 697, 29899, 517, 29899, 650, 1156, 263, 4450, 300, 837, 1312, 286, 1337, 537, 2635, 29889, 1152, 1342, 29892, 285, 29891, 3308, 12696, 29900, 29929, 29906, 29945, 18897, 526, 337, 311, 331, 519, 363, 3148, 12696, 1156, 3839, 29871, 29906, 29945, 386, 29892, 29871, 29906, 29900, 29906, 29896, 29889, 7806, 285, 29891, 6066, 286, 1337, 537, 756, 385, 6942, 30081, 1068, 29943, 29891, 6066, 1068, 30081, 1001, 29907, 29899, 29906, 29900, 15040, 8078, 29889, 13, 13, 9760, 1134, 310, 5321, 1008, 284, 297, 612, 969, 1122, 367, 409, 1119, 14561, 964, 263, 13587, 8078, 470, 30081, 1068, 17242, 1068, 30081, 1454, 393, 5321, 1008, 284, 29889, 3650, 262, 8078, 29879, 7150, 21608, 515, 4160, 322, 3787, 372, 2745, 372, 338, 6206, 515, 278, 1788, 29889, 3650, 262, 947, 451, 2407, 738, 325, 1292, 2106, 29889, 3650, 262, 3635, 1169, 4160, 304, 11013, 27942, 599, 21608, 372, 8640, 29889, 13, 13, 13, 7686, 29995, 673, 13, 10401, 366, 7246, 263, 716, 5988, 773, 278, 421, 3258, 17242, 29952, 740, 29892, 366, 674, 7150, 278, 3211, 310, 278, 15141, 2825, 5988, 8078, 29889, 910, 338, 9875, 491, 278, 421, 7122, 29952, 736, 995, 297, 278, 740, 29889, 13, 13, 28956, 2929, 333, 537, 13, 29871, 740, 1653, 17242, 29898, 13, 1678, 3211, 24342, 13, 29871, 1723, 7029, 3639, 313, 7328, 5988, 29897, 13, 28956, 13, 13, 797, 278, 3030, 310, 612, 969, 1019, 5770, 29892, 278, 5988, 3211, 338, 1407, 4100, 29889, 7806, 1134, 310, 5321, 1008, 284, 297, 612, 969, 1122, 367, 409, 1119, 14561, 964, 263, 13587, 8078, 470, 3579, 17242, 1068, 363, 393, 5321, 1008, 284, 29889, 3650, 262, 8078, 29879, 7150, 21608, 515, 4160, 322, 3787, 372, 2745, 372, 338, 6206, 515, 278, 1788, 29889, 29871, 13, 13, 8439, 1079, 29892, 278, 5988, 3211, 366, 7150, 508, 367, 1304, 304, 16254, 411, 278, 5988, 8078, 29889, 1152, 1342, 29892, 366, 508, 19754, 277, 21608, 964, 278, 5988, 8078, 29892, 470, 28679, 21608, 515, 372, 29889, 29871, 13, 13, 7301, 1096, 393, 278, 3650, 262, 8078, 947, 451, 2407, 738, 325, 1292, 2106, 29889, 739, 3763, 8640, 278, 21608, 2745, 896, 526, 6206, 515, 278, 1788, 29889, 450, 10643, 310, 278, 2989, 11747, 17437, 310, 263, 325, 1292, 29892, 515, 4969, 263, 325, 1292, 29892, 4417, 322, 11077, 5321, 1008, 284, 29892, 4417, 322, 11077, 2553, 29873, 29892, 8454, 5321, 1008, 284, 2133, 29892, 3635, 5367, 23904, 362, 310, 1090, 22017, 1008, 284, 1891, 325, 1292, 29879, 29892, 322, 27777, 5321, 1008, 284, 322, 2553, 29873, 304, 263, 716, 3652, 29892, 338, 2309, 491, 278, 3579, 26270, 16722, 1617, 1068, 8078, 29889, 29871, 13, 13, 6295, 29892, 297, 15837, 29892, 278, 5988, 3211, 366, 7150, 515, 278, 421, 3258, 17242, 29952, 740, 338, 278, 3211, 310, 278, 8078, 988, 366, 508, 19754, 277, 470, 28679, 21608, 29889, 2, 1, 1273, 7686, 2799, 4080, 13, 5618, 947, 278, 19392, 29886, 473, 29952, 740, 437, 29973, 13, 13, 7686, 29995, 15228, 13, 29937, 9243, 16722, 1617, 13, 13, 2277, 29937, 903, 29886, 473, 13, 13, 28956, 2929, 333, 537, 13, 2220, 903, 29886, 473, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 2281, 3630, 10562, 29889, 29963, 1292, 325, 1292, 3383, 2281, 3630, 10562, 29889, 22031, 2925, 6411, 2925, 3383, 2281, 3630, 10562, 29889, 19204, 3652, 3383, 938, 29896, 29906, 29947, 297, 29895, 29892, 938, 29896, 29906, 29947, 1616, 29897, 7463, 3639, 313, 4984, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 2528, 5321, 1008, 284, 322, 27942, 515, 325, 1292, 29892, 8206, 21608, 515, 322, 5503, 27942, 287, 24342, 304, 1404, 13, 2816, 29892, 1634, 388, 304, 325, 1292, 322, 3349, 5321, 1008, 284, 29892, 8206, 27942, 287, 24342, 515, 322, 5503, 21608, 304, 1404, 29918, 13, 13, 2277, 29937, 1671, 13, 13, 28956, 2929, 333, 537, 13, 2220, 1671, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 938, 29896, 29906, 29947, 297, 29895, 29892, 938, 29896, 29906, 29947, 1616, 29897, 7029, 6901, 3639, 313, 4984, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 2517, 666, 5987, 263, 325, 1292, 29892, 5662, 3864, 372, 338, 5321, 1008, 284, 1891, 12335, 29889, 13, 1762, 367, 1304, 491, 2553, 29873, 10643, 8078, 29879, 3032, 13, 13, 2277, 29937, 2243, 332, 29886, 13, 13, 28956, 2929, 333, 537, 13, 2220, 2243, 332, 29886, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 13122, 29896, 29906, 29947, 297, 29895, 29892, 13122, 29896, 29906, 29947, 1616, 29897, 7029, 3639, 313, 4984, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 29934, 6085, 346, 2553, 29873, 322, 5321, 1008, 284, 515, 263, 325, 1292, 29892, 5330, 8253, 5321, 1008, 284, 2133, 12747, 29889, 13, 1762, 367, 1304, 491, 23904, 362, 24000, 3032, 13, 13, 2277, 29937, 9679, 13, 13, 28956, 2929, 333, 537, 13, 2220, 9679, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 6262, 29953, 716, 19204, 1204, 29892, 938, 29896, 29906, 29947, 1616, 29897, 7029, 3639, 313, 4984, 3630, 10562, 29889, 29963, 1292, 29892, 2281, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 7277, 3652, 322, 2553, 29873, 310, 263, 325, 1292, 29889, 13, 1576, 3883, 5432, 445, 740, 884, 4225, 304, 15649, 14407, 297, 278, 11565, 363, 278, 716, 3652, 29892, 322, 19417, 372, 297, 11565, 363, 278, 2030, 3652, 3032, 13, 13, 2277, 29937, 3233, 13, 13, 28956, 2929, 333, 537, 13, 2220, 3233, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29897, 7029, 3639, 313, 524, 29906, 29945, 29953, 29897, 13, 28956, 13, 13, 29918, 11609, 278, 5321, 1008, 284, 2133, 3233, 310, 263, 325, 1292, 29889, 739, 674, 367, 8178, 565, 1090, 22017, 1008, 284, 1891, 3032, 13, 13, 2277, 29937, 286, 1535, 13, 13, 28956, 2929, 333, 537, 13, 2220, 286, 1535, 29898, 13193, 29953, 3652, 1204, 29897, 7029, 13, 28956, 13, 13, 29918, 9182, 278, 27942, 292, 6554, 472, 286, 1337, 537, 363, 263, 3652, 29918, 13, 13, 2277, 29937, 903, 29885, 1535, 13, 13, 28956, 2929, 333, 537, 13, 2220, 903, 29885, 1535, 29898, 13193, 29953, 3652, 1204, 29892, 2281, 3630, 10562, 29889, 19204, 3652, 19925, 7463, 13, 28956, 13, 13, 29918, 9182, 278, 27942, 292, 6554, 472, 286, 1337, 537, 363, 263, 3652, 29918, 13, 13, 2277, 29937, 1035, 582, 284, 13, 13, 28956, 2929, 333, 537, 13, 2220, 1035, 582, 284, 29898, 13193, 29953, 3652, 1204, 29897, 7029, 3639, 313, 13470, 29906, 29945, 29953, 29897, 13, 28956, 13, 13, 29918, 8015, 29878, 2418, 278, 6554, 1035, 582, 284, 1951, 286, 1337, 537, 29892, 286, 19021, 565, 5181, 3032, 13, 29937, 9243, 16722, 1617, 13, 13, 2277, 29937, 903, 29886, 473, 13, 13, 28956, 2929, 333, 537, 13, 2220, 903, 29886, 473, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 2281, 3630, 10562, 29889, 29963, 1292, 325, 1292, 3383, 2281, 3630, 10562, 29889, 22031, 2925, 6411, 2925, 3383, 2281, 3630, 10562, 29889, 19204, 3652, 3383, 938, 29896, 29906, 29947, 297, 29895, 29892, 938, 29896, 29906, 29947, 1616, 29897, 7463, 3639, 313, 4984, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 2528, 5321, 1008, 284, 322, 27942, 515, 325, 1292, 29892, 8206, 21608, 515, 322, 5503, 27942, 287, 24342, 304, 1404, 13, 2816, 29892, 1634, 388, 304, 325, 1292, 322, 3349, 5321, 1008, 284, 29892, 8206, 27942, 287, 24342, 515, 322, 5503, 21608, 304, 1404, 29918, 13, 13, 2277, 29937, 1671, 13, 13, 28956, 2929, 333, 537, 13, 2220, 1671, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 938, 29896, 29906, 29947, 297, 29895, 29892, 938, 29896, 29906, 29947, 1616, 29897, 7029, 6901, 3639, 313, 4984, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 2517, 666, 5987, 263, 325, 1292, 29892, 5662, 3864, 372, 338, 5321, 1008, 284, 1891, 12335, 29889, 13, 1762, 367, 1304, 491, 2553, 29873, 10643, 8078, 29879, 3032, 13, 13, 2277, 29937, 2243, 332, 29886, 13, 13, 28956, 2929, 333, 537, 13, 2220, 2243, 332, 29886, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 13122, 29896, 29906, 29947, 297, 29895, 29892, 13122, 29896, 29906, 29947, 1616, 29897, 7029, 3639, 313, 4984, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 29934, 6085, 346, 2553, 29873, 322, 5321, 1008, 284, 515, 263, 325, 1292, 29892, 5330, 8253, 5321, 1008, 284, 2133, 12747, 29889, 13, 1762, 367, 1304, 491, 23904, 362, 24000, 3032, 13, 13, 2277, 29937, 9679, 13, 13, 28956, 2929, 333, 537, 13, 2220, 9679, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29892, 6262, 29953, 716, 19204, 1204, 29892, 938, 29896, 29906, 29947, 1616, 29897, 7029, 3639, 313, 4984, 3630, 10562, 29889, 29963, 1292, 29892, 2281, 3630, 10562, 29889, 22031, 2925, 29897, 13, 28956, 13, 13, 29918, 7277, 3652, 322, 2553, 29873, 310, 263, 325, 1292, 29889, 13, 1576, 3883, 5432, 445, 740, 884, 4225, 304, 15649, 14407, 297, 278, 11565, 363, 278, 716, 3652, 29892, 322, 19417, 372, 297, 11565, 363, 278, 2030, 3652, 3032, 13, 13, 2277, 29937, 3233, 13, 13, 28956, 2929, 333, 537, 13, 2220, 3233, 29898, 13193, 29896, 29906, 325, 1292, 1204, 29897, 7029, 3639, 313, 524, 29906, 29945, 29953, 29897, 13, 28956, 13, 13, 29918, 11609, 278]}\n",
      "2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n| ---- | ---- | ----------- |\\n| [0] | address | The token contract address |\\n\\n### fee\\n\\n```solidity\\nfunction fee() external view returns (uint24)\\n```\\n\\nThe pool\\'s fee in hundredths of a bip, i.e. 1e-6\\n\\n| Name | Type | Description |\\n| ---- | ---- | ----------- |\\n| [0] | uint24 | The fee |\\n\\n### tickSpacing\\n\\n```solidity\\nfunction tickSpacing() external view returns (int24)\\n```\\n\\nThe pool tick spacing\\n\\n_Ticks can only be used at multiples of this value, minimum of 1 and always positive\\ne.g.: a tickSpacing of 3 means ticks can be initialized every 3rd tick, i.e., ..., -6, -3, 0, 3, 6, ...\\nThis value is an int24 to avoid casting even though it is always positive._\\n\\n| Name | Type | Description |\\n| ---- | ---- | ----------- |\\n| [0] | int24 | The tick spacing |\\n\\n\\n%%% Answer\\nThe \\'fee\\' method in the IUniswapV3PoolImmutables interface returns the pool\\'s fee in hundredths of a bip, i.e. 1e-6. This is represented as a `uint24` data type. \\n\\nHere is the method signature:\\n```solidity\\nfunction fee() external view returns (uint24)\\n```\\n\\nThis fee is a fixed parameter for a pool and will always return the same value.</s><s> %%% Instruction\\nWhat is the purpose of the ERC1155Mock contract?\\n\\n%%% Context\\n# ERC1155Mock\\n\\n# Solidity API\\n\\n## ERC1155Mock\\n\\n### uri\\n\\n```solidity\\nfunction uri(uint256) public view virtual returns (string)\\n```\\n\\n### mint\\n\\n```solidity\\nfunction mint(address to, uint256 id, uint256 amount, bytes data) external\\n```\\n# ERC1155Mock\\n\\n# Solidity API\\n\\n## ERC1155Mock\\n\\n### uri\\n\\n```solidity\\nfunction uri(uint256) public view virtual returns (string)\\n```\\n\\n### mint\\n\\n```solidity\\nfunction mint(address to, uint256 id, uint256 amount, bytes data) external\\n```\\n\\n\\n%%% Answer\\nThe `ERC1155Mock` contract is a mock contract used for testing purposes in the Yield Protocol. It provides a simplified implementation of the ERC1155 token standard, which is a multi-token standard that allows a single contract to produce any number of tokens.\\n\\nThe `ERC1155Mock` contract includes the `uri` function, which returns a URI for a given token ID, and the `mint` function, which allows for the creation of new tokens. \\n\\nPlease note that this contract is likely used in a testing environment and may not be used in the actual Yield Protocol\\'s mainnet or production environment.</s><s> %%% Instruction\\nWhat is the role of the operations team in the Yield protocol?\\n\\n%%% Context\\n# smart_contracts_overview\\n\\n## AccessControl\\n\\nThe access control contract was adapted from OpenZeppelin\\'s AccessControl.sol and is inherited from most other contracts in the Yield Protocol.\\n\\nA role exists implicitly for each function in a contract, with the ROOT role as the admin for the role.\\n\\nIf the `auth` modifier is present in a function, access must have been granted to the caller by an account with the admin role for the function role. This admin role will usually be ROOT, but that can be changed.\\n\\nAn `admin` modifier exists to restrict functions to accounts bearing the `admin` role of a given other role. This is not used outside AccessControl.sol.\\n\\nAn account belonging to the admin role for a function can grant and revoke memberships to the function role.\\n\\nThe ROOT role is special in that it is its own admin so that any member of ROOT can grant and revoke ROOT permissions on other accounts.\\n\\nThere is a special LOCK role, that is also an admin of itself, but has no members. By changing the admin role of a function role to LOCK, no further changes can ever be done to the function role membership, except users voluntarily renouncing to the function role.\\n\\n\\n## ERC20 and ERC20Permit\\n\\nThe Yield Protocol ERC20 contracts borrow heavily from the DS-Token implementation. The ERC2162 extension is taken from WETH10, which was taken in turn from Yield v1.\\n\\n\\n## Timelock\\n\\nThe Yield Protocol uses its own implementation of a Timelock, derived from Compound’s original contract, but inheriting from AccessControl and implementing a different pattern to set the earliest time that an execution can be done.\\n\\n\\n## EmergencyBrake\\n\\nThe EmergencyBrake stores the instructions to remove the orchestration between contracts, which is intended to be used in emergency situations to easily isolate parts of the protocol.\\n\\n[Edit this page](https://github.com/yieldprotocol/docs-v2/edit/main/developers/smart_contracts_overview.md)\\n# faq\\n\\n### How is Yield Governed?\\n\\nThe founding team believes in decentralization and building towards community control of the protocol. Right now, we believe that is best served by building, and growing community involvement. Long term we expect Yield Protocol to be community-owned and community controlled through a process of progressive decentralization.\\n\\n[Edit this page](https://github.com/yieldprotocol/docs-v2/edit/main/faq.md)\\n\\n\\n%%% Answer\\nThe operations team in the Yield Protocol plays a crucial role in executing what is determined by governance. They have limited control and their permissions are channeled through the AccessControl contract, which is implemented as a 1-of-n multisig. This means that individual members can directly be given permissions consistent with an operations role.\\n\\nThe operations team is responsible for submitting proposals for changes to the protocol. These proposals are a series of function calls to be executed from the Timelock. Once a proposal is submitted, it can be independently decoded and verified by the owners of the multisig or the community. If the proposal is approved, the operations team will then execute the approved proposal, no earlier than the delay set in the Timelock.\\n\\nIn the future, an additional x-of-n multisig could be considered for the operations role. The operations role can also be subdivided into several roles, with different permissions and parameters, as the protocol grows.\\n\\nThis is quoted from the context: \"The operations team has limited control to execute what is determined by governance. The AccessControl contract is implemented as a 1-of-n multisig and so individual members can directly be given permissions consistent with an operations role. In the future, an additional x-of-n multisig could be considered for the operations role. The operations role can also be subdivided into several roles, with different permissions and parameters.\"</s><s> %%% Instruction\\nHow is the liquidation process triggered?\\n\\n%%% Context\\n# faq\\n\\n### What is Liquidation?\\n\\nBorrowers must maintain a minimum amount of collateral in their vault to secure the debt they owe. If a borrower fails to do so, their vault may be liquidated. Their collateral will be seized and auctioned off to repay their debts.\\n\\n### How does liquidation process work?\\n\\nWhen the value of the collateral in a borrowing position becomes less than the value of the debt times the collateralization ratio, the position will be put up for auction. Liquidators will repay the debt in exchange for the collateral until there is no debt left. The borrowing position (vault) will be returned to the original owner with any collateral left after the liquidators have repaid all the debt.\\n\\n### Who liquidates positions?\\n\\nAnyone may liquidate an insufficiently collateralized borrowing position.\\n\\n## Security\\n\\n### Is Yield Protocol audited?\\n\\nYes! Yield Protocol was audited by [Code 423n4](https://code423n4.com). You may find the report [here]().\\n# liquidations\\n\\n## Liquidations Process\\nThe liquidations process is managed by the [Witch](https://github.com/yieldprotocol/vault-v2/blob/488b12fe586d745a51090ea9651211e79d686f8b/packages/'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lm_dataset[0])\n",
    "print(len(lm_dataset[0]['input_ids']))\n",
    "tokenizer.decode(lm_dataset[34]['input_ids']) # concatenating different samples break coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/llama/yield/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-Tune LLaMA 13B with QLoRA on Amazon SageMaker\n",
    "\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2106.09685)\" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is: \n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "We prepared a [run_clm.py](./scripts/run_clm.py), which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code. The model will be temporally offloaded to disk, if it is too large to fit into memory.\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n",
    "\n",
    "### Harwarde requirements\n",
    "\n",
    "We also ran several experiments to determine, which instance type can be used for the different model sizes. The following table shows the results of our experiments. The table shows the instance type, model size, context length, and max batch size. \n",
    "\n",
    "| Model        | Instance Type     | Max Batch Size | Context Length |\n",
    "|--------------|-------------------|----------------|----------------|\n",
    "| [LLama 7B]() | `(ml.)g5.4xlarge` | `3`            | `2048`         |\n",
    "| [LLama 13B]() | `(ml.)g5.4xlarge` | `2`            | `2048`         |\n",
    "| [LLama 70B]() | `(ml.)p4d.24xlarge` | `1++` (need to test more configs)            | `2048`         |\n",
    "\n",
    "\n",
    "> You can also use `g5.2xlarge` instead of the `g5.4xlarge` instance type, but then it is not possible to use `merge_weights` parameter, since to merge the LoRA weights into the model weights, the model needs to fit into memory. But you could save the adapter weights and merge them using [merge_adapter_weights.py](./scripts/merge_adapter_weights.py) after training.\n",
    "\n",
    "_Note: We plan to extend this list in the future. feel free to contribute your setup!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 8,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 400,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    max_run              =  432000,\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example for LLaMA 13B, the SageMaker training job took `120k seconds`, which is about `32 hours`. The ml.g5.4xlarge instance we used costs `$2.03 per hour` for on-demand usage. As a result, the total cost for training our fine-tuned LLaMa 2 model was only ~`$64`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps \n",
    "\n",
    "You can deploy your fine-tuned LLaMA model to a SageMaker endpoint and use it for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
