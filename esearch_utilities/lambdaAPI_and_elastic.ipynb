{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca5047-51d3-4b79-a11b-b37c1f477e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain==0.0.230 eland elasticsearch huggingface-hub tqdm requests ipython GitPython\n",
    "!pip install \"transformers==4.31.0\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet\n",
    "!pip install \"sagemaker>=2.163.0\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea82e1-83a5-4fa7-ba63-ad2d67e38a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from pathlib import Path\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f5707-272c-4ca2-b09c-0090bc55c814",
   "metadata": {
    "id": "uQ5yoa_r8opV"
   },
   "source": [
    "Now we create a prompt chain that gets the most relevant passage from Elasticsearch using a vector search, and then uses that knowledge in the prompt to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976e2192-658f-4236-bf35-06e466f922e2",
   "metadata": {
    "id": "uQ5yoa_r8opV"
   },
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3be1ac-f663-404a-8658-316e9e0ca47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7d242-0cc6-43f2-8e3a-f1de1e941c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub. Data is private, remember to authenticate your Huggingface CLI token that's added to Yield organization.\n",
    "data_files = {\"train\": \"qa_pairs_train.jsonl\", \"test\":\"qa_pairs_test.jsonl\"}\n",
    "dataset = load_dataset(\"YieldInc/chatbot_qa_dataset_splitted\", data_files=data_files)['test']\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03da5e5-7ae5-4f6e-9325-2c11f36e0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_cloud_id = getpass.getpass('Enter Elastic Cloud ID:  ')\n",
    "es_user = getpass.getpass('Enter cluster username:  ')\n",
    "es_pass = getpass.getpass('Enter cluster password:  ')\n",
    "endpoint = getpass.getpass('Enter the URL for the Elastic Cloud Deployment Instance: ')\n",
    "es_url =  f\"https://{es_user}:{es_pass}@{endpoint}:443\"\n",
    "\n",
    "index_name = getpass.getpass('Enter the document database index:  ')\n",
    "es = Elasticsearch(cloud_id=es_cloud_id,\n",
    "                   basic_auth=(es_user, es_pass)\n",
    "                   )\n",
    "print(es.info()) # should return cluster info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ab374-ee80-4e19-b8b4-816d5edc9264",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"baai__bge-base-en\" \n",
    "# this refers to the model string appeared in Machine Learning models section under 'Machine Learning' on the ElasticSearch Kibana Panel\n",
    "\n",
    "def search_question(query_text):\n",
    "    # Elasticsearch query (BM25) and kNN configuration for hybrid search\n",
    "    print(\"Query text is\", query_text)\n",
    "    query = {\n",
    "        \"bool\": {\n",
    "            \"must\": [{\n",
    "                \"match\": {\n",
    "                    \"text\": {\n",
    "                        \"query\": query_text,\n",
    "                        \"boost\": 15\n",
    "                    }\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    knn = {\n",
    "        \"field\": \"title-vector\",\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 20,\n",
    "        \"query_vector_builder\": {\n",
    "            \"text_embedding\": {\n",
    "                \"model_id\": model_id,\n",
    "                \"model_text\": query_text\n",
    "            }\n",
    "        },\n",
    "        \"boost\": 5\n",
    "    } # boost is defining relative importance of keyword search and kNN vector retrieval search.\n",
    "\n",
    "    fields = [\"text\"] # define the field in document collection to match against query.\n",
    "    index = index_name\n",
    "    resp = es.search(index=index,\n",
    "                     query=query,\n",
    "                     knn=knn,\n",
    "                     fields=fields,\n",
    "                     size=5,\n",
    "                     source=True)\n",
    "    \n",
    "    # print(\"Query is\", query)\n",
    "    # print(\"Response is\", resp)\n",
    "    body = [resp['hits']['hits'][i]['fields']['text'][0] for i in range(len(resp['hits']['hits']))] # return all relevant docs here\n",
    "    sources = [resp['hits']['hits'][i]['_source']['metadata']['source'] for i in range(len(resp['hits']['hits']))]\n",
    "    return body, sources\n",
    "\n",
    "def format_yield_validate(sample, relevant_documents):\n",
    "    instruction = f\"%%% Instruction\\n{sample['question']}\"\n",
    "    docs = '\\n'.join(relevant_documents)\n",
    "    context = f\"%%% Context\\n{docs}\"\n",
    "    response = f\"%%% Answer\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6f9e3-ba19-4577-8573-977759950afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[randrange(len(dataset))] # random QA entry\n",
    "relevant_docs, relevant_sources = search_question(sample['question'])\n",
    "prompt = format_yield_validate(sample, relevant_docs)\n",
    "print(prompt)\n",
    "\n",
    "# hyperparameters for llm\n",
    "payload = {\n",
    "  \"inputs\":  prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "output = llm.predict(payload)[0]['generated_text']\n",
    "print(f\"GENERATED ANSWER: {output}\\n\\n\")\n",
    "print(f\"REFERENCE ANSWER: {sample['answer']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23ae08-1d04-45ac-8375-48397e356e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly test it on the Lambda API.\n",
    "import urllib.parse\n",
    "from urllib.parse import quote\n",
    "\n",
    "url = getpass.getpass(\"Enter the Lambda function's URL from function configuration on AWS Lambda panel: \")\n",
    "sample = dataset[randrange(len(dataset))]\n",
    "relevant_docs, relevant_srcs = search_question(sample['question'])\n",
    "prompt = format_yield_validate(sample, relevant_docs)\n",
    "print(prompt)\n",
    "\n",
    "# URL-encode the query to the actual format, around 3k tokens max, could be errorneous\n",
    "encoded_query = urllib.parse.quote(prompt)\n",
    "full_url = f\"{url}?query={encoded_query}\"\n",
    "print(full_url)\n",
    "\n",
    "# Send GET request to your Lambda function\n",
    "response = requests.get(full_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response\n",
    "    answer = json.loads(response.text)\n",
    "\n",
    "    # Send the answer to the Discord channel\n",
    "    print(f\"Answer:{answer}\\n\")\n",
    "    print(f\"Sources:{relevant_srcs}\")\n",
    "    print(f\"Reference Answer:{sample['answer']}\\n\")\n",
    "else:\n",
    "    print(f\"{response.status_code} An error occurred while processing your request.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
